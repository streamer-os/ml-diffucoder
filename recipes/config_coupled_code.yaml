# Model arguments
model_name_or_path: /path-to-model # e.g. apple/DiffuCoder-7B-Instruct
torch_dtype: bfloat16
trust_remote_code: true
# Data training arguments
dataset_name: /path-to-dataset # e.g. ./recipes/acecode_hard.jsonl
dataset_prompt_column: question
system_prompt: "You are a helpful assistant."

# GRPO trainer config
sync_ref_model: True
ref_model_sync_steps: 64
beta: 0.01
epsilon: 0.5
scale_rewards: false
bf16: true
use_vllm: false
do_eval: false
num_generations: 10
num_iterations: 2
per_device_eval_batch_size: 1
per_device_train_batch_size: 5
gradient_accumulation_steps: 2
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

learning_rate: 1.0e-06
adam_beta1: 0.9
adam_beta2: 0.99
weight_decay: 0.1
max_grad_norm: 0.2
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 10
logging_strategy: steps
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
num_train_epochs: 1
output_dir: /path-to-output
overwrite_output_dir: true
e2b_router_url: 0.0.0.0:8000

## generation config
max_prompt_length: 200
max_completion_length: 256
random_masking: True
p_mask_prompt: 0.0
diffusion_steps: 256
generation_batch_size: 10
generation_temperature: 1.0

report_to:
- wandb
wandb_entity: "your-wandb-entity"
wandb_project: "open-r1-code"
reward_funcs:
- code
- code_format
reward_weights:
- 2.0
- 0.5
save_strategy: "steps"
save_steps: 2000
save_total_limit: 5
seed: 42
warmup_ratio: 0.0001